# Deep Learning Research Papers Implementations
This repository contains implementations of various deep learning research papers. Each implementation is a standalone project that aims to reproduce and provide a practical implementation of the techniques proposed in the corresponding research papers.

##Implemented Papers
This has implementations of the following research papers:

- Deep Residual Learning for Image Recognition

Implementation Directory: /resnet
Description: The Deep Residual Learning for Image Recognition paper introduced the ResNet (Residual Network), a deep convolutional neural network architecture that addressed the problem of training very deep neural networks. The authors proposed the concept of residual learning, utilizing residual blocks with skip connections to enable networks to learn residual functions. By learning the difference between the input and output of a block, rather than the complete transformation, ResNet allowed gradients to flow directly through the network without vanishing or exploding. This breakthrough approach facilitated the training of extremely deep networks and significantly improved performance on image classification benchmarks. ResNet became a widely adopted architecture, revolutionizing the field of deep learning and paving the way for deeper and more accurate convolutional neural networks in image recognition tasks.

- Attention is all you need

Implementation Directory: attention/
Description: The "Attention Is All You Need" paper introduced the Transformer architecture, a groundbreaking neural network model for natural language processing. By leveraging self-attention mechanisms instead of recurrent or convolutional layers, the Transformer achieved state-of-the-art results in sequence-to-sequence tasks, such as machine translation. This approach enabled the model to capture long-range dependencies and eliminated the need for sequential processing, leading to faster training and inference times. The Transformer architecture revolutionized NLP by showcasing the power of attention mechanisms in processing sequential data.
